{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3F3 Random Variables and Random Number Generation Lab\n",
    "\n",
    "## Objectives\n",
    "- Understand random variables and functions of random variables\n",
    "- Study the Jacobian as used with random variables\n",
    "- Experiment with methods for non-uniform random number generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import erfc\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create img directory for saving plots\n",
    "if not os.path.exists(\"img\"):\n",
    "    os.makedirs(\"img\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "plt.rcParams[\"font.size\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Density Estimation (KDE)\n",
    "\n",
    "The **kernel density function** (or kernel density estimator) is defined as:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{\\sigma}\\mathcal{K}\\left(\\frac{x - x^{(i)}}{\\sigma}\\right)$$\n",
    "\n",
    "where:\n",
    "- $N$ is the number of samples\n",
    "- $x^{(i)}$ are the observed data points\n",
    "- $\\mathcal{K}(u)$ is the kernel function (commonly Gaussian kernel)\n",
    "- $\\sigma$ is the bandwidth (smoothing parameter)\n",
    "\n",
    "The **Gaussian kernel** specifically is:\n",
    "\n",
    "$$\\mathcal{K}_{\\text{Gauss}}(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{u^2}{2}\\right)$$\n",
    "\n",
    "resulting in the KDE:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{N\\sigma}\\sum_{i=1}^{N}\\exp\\left(-\\frac{(x - x^{(i)})^2}{2\\sigma^2}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksdensity(data, width=0.3, x_axis=None):\n",
    "    \"\"\"\n",
    "    Kernel smoothing density estimator using Gaussian kernel.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Data points from which to estimate the density\n",
    "    width : float\n",
    "        Bandwidth (sigma) for the Gaussian kernel\n",
    "    x_axis : array-like, optional\n",
    "        Points at which to evaluate the density. If None, creates default range\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    x_values : ndarray\n",
    "        Points at which density was evaluated\n",
    "    pdf : ndarray\n",
    "        Estimated probability density at x_values\n",
    "    \"\"\"\n",
    "\n",
    "    if x_axis is None:\n",
    "        x_axis = np.linspace(np.min(data) - 3 * width, np.max(data) + 3 * width, 200)\n",
    "\n",
    "    # Gaussian kernel: K(u) = (1/sqrt(2π)) * exp(-u²/2)\n",
    "    def normal_pdf(x, mu=0.0, sigma=1.0):\n",
    "        u = (x - mu) / abs(sigma)\n",
    "        return (1 / (np.sqrt(2 * np.pi) * abs(sigma))) * np.exp(-u * u / 2)\n",
    "\n",
    "    # Apply kernel to each data point and average\n",
    "    pdf = np.zeros_like(x_axis, dtype=float)\n",
    "    for xi, x in enumerate(x_axis):\n",
    "        # Contribution from each data point using Gaussian kernel\n",
    "        pdf[xi] = np.mean([normal_pdf(x, mu=d, sigma=width) for d in data])\n",
    "\n",
    "    return x_axis, pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform and Normal Random Variables\n",
    "\n",
    "Generate samples and plot histograms with theoretical PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random samples\n",
    "N = 1000\n",
    "\n",
    "# Gaussian: N(0, 1)\n",
    "x_normal = np.random.randn(N)\n",
    "\n",
    "# Uniform: U(0, 1)\n",
    "x_uniform = np.random.rand(N)\n",
    "\n",
    "print(\n",
    "    f\"Generated {N} normal samples: mean={x_normal.mean():.4f}, std={x_normal.std():.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Generated {N} uniform samples: mean={x_uniform.mean():.4f}, std={x_uniform.std():.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Histogram of Gaussian random numbers overlaid on exact Gaussian curve (scaled):\n",
    ">\n",
    "> Kernel density estimate for Gaussian random numbers overlaid on exact Gaussian curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: NORMAL DISTRIBUTION\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "counts, bins, patches = ax.hist(\n",
    "    x_normal,\n",
    "    bins=20,\n",
    "    density=True,\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Histogram\",\n",
    ")\n",
    "\n",
    "# Overlay theoretical PDF\n",
    "x_theory = np.linspace(-4, 4, 200)\n",
    "p_theory = stats.norm.pdf(x_theory, loc=0, scale=1)\n",
    "ax.plot(x_theory, p_theory, \"r-\", linewidth=2.5, label=\"Theoretical N(0,1)\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(\"Normal Distribution: Histogram vs Theory\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Kernel Density Estimation\n",
    "ax = axes[1]\n",
    "bandwidth_normal = 0.2\n",
    "x_kde, pdf_kde = ksdensity(x_normal, width=bandwidth_normal, x_axis=x_theory)\n",
    "ax.plot(\n",
    "    x_kde, pdf_kde, \"b-\", linewidth=2.5, label=f\"KDE (bandwidth={bandwidth_normal})\"\n",
    ")\n",
    "ax.plot(x_theory, p_theory, \"r-\", linewidth=2.5, label=\"Theoretical N(0,1)\")\n",
    "ax.scatter(x_normal[::50], np.zeros(len(x_normal[::50])), alpha=0.3, s=20, color=\"blue\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(\"Normal Distribution: KDE vs Theory\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/normal_histogram_kde.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Histogram of Uniform random numbers overlaid on exact Uniform curve (scaled):\n",
    ">\n",
    "> Kernel density estimate for Uniform random numbers overlaid on exact uniform curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: UNIFORM DISTRIBUTION\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "counts, bins, patches = ax.hist(\n",
    "    x_uniform,\n",
    "    bins=20,\n",
    "    density=True,\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Histogram\",\n",
    ")\n",
    "\n",
    "# Overlay theoretical PDF\n",
    "x_theory_u = np.linspace(-0.3, 1.3, 200)\n",
    "p_theory_u = stats.uniform.pdf(x_theory_u, loc=0, scale=1)\n",
    "ax.plot(x_theory_u, p_theory_u, \"r-\", linewidth=2.5, label=\"Theoretical U(0,1)\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(\"Uniform Distribution: Histogram vs Theory\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1.5])\n",
    "\n",
    "# Kernel Density Estimation\n",
    "ax = axes[1]\n",
    "bandwidth_uniform = 0.05\n",
    "x_kde_u, pdf_kde_u = ksdensity(x_uniform, width=bandwidth_uniform, x_axis=x_theory_u)\n",
    "ax.plot(\n",
    "    x_kde_u,\n",
    "    pdf_kde_u,\n",
    "    \"b-\",\n",
    "    linewidth=2.5,\n",
    "    label=f\"KDE (bandwidth={bandwidth_uniform})\",\n",
    ")\n",
    "ax.plot(x_theory_u, p_theory_u, \"r-\", linewidth=2.5, label=\"Theoretical U(0,1)\")\n",
    "ax.scatter(\n",
    "    x_uniform[::50], np.zeros(len(x_uniform[::50])), alpha=0.3, s=20, color=\"blue\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(\"Uniform Distribution: KDE vs Theory\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/uniform_histogram_kde.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Comparison of Histogram vs Kernel Density Estimation\n",
    "\n",
    "> Comment on the advantages and disadvantages of the kernel density method compared with the histogram method for estimation of a probability density from random samples:\n",
    "\n",
    "### Theoretical Convergence Properties\n",
    "\n",
    "From statistical theory, both histogram and KDE methods have similar bias-variance structure, but differ in asymptotic convergence rates:\n",
    "\n",
    "**Histogram Method:**\n",
    "- Mean Squared Error (MSE): $O(h^4) + O(1/(Nh))$ where $h = 1/M$ (bin width)\n",
    "- Optimal bin width: $M_{opt} \\propto N^{1/3}$\n",
    "- **Optimal convergence rate: $O(N^{-2/3})$**\n",
    "\n",
    "**Kernel Density Estimation:**\n",
    "- Mean Squared Error (MSE): $O(h^4) + O(1/(Nh))$ (same structure)\n",
    "- Optimal bandwidth: $h_{opt} \\propto N^{-1/5}$\n",
    "- **Optimal convergence rate: $O(N^{-4/5})$** — **FASTER than histogram!**\n",
    "\n",
    "### Method Comparison Summary\n",
    "\n",
    "| Aspect | Histogram | KDE |\n",
    "|--------|-----------|-----|\n",
    "| **Visual Form** | Bars (discrete) | Smooth curve |\n",
    "| **Interpretation** | Exact bin counts (intuitive) | Smoothed estimate |\n",
    "| **Bin/Bandwidth** | Sensitive to bin width and position | Less sensitive to bandwidth choice |\n",
    "| **Boundary Effects** | Respects finite support (e.g., [0,1]) | May smooth beyond boundaries |\n",
    "| **Convergence Rate** | $O(N^{-2/3})$ | $O(N^{-4/5})$ (better) |\n",
    "| **Computational Cost** | Trivial | $O(N)$ kernel evaluations |\n",
    "| **Multimodal Detection** | Difficult with wrong bin size | Better at revealing structure |\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Histogram Advantages:**\n",
    "- Directly represents actual sample counts per bin (intuitive)\n",
    "- Directly observable from data: histogram height $\\propto n_j/\\delta$ approximates probability density\n",
    "- Respects finite support of distributions (e.g., [0,1] for uniform)\n",
    "- Computationally simple and efficient\n",
    "\n",
    "**Histogram Disadvantages:**\n",
    "- Visual appearance sensitive to bin width $\\delta$ and bin position selection\n",
    "- Artificial discontinuities between bins\n",
    "\n",
    "**KDE Advantages:**\n",
    "- Smooth, continuous estimate that better reveals density structure\n",
    "- Less sensitive to bin width selection (bandwidth) compared to histogram's bin width effects\n",
    "- Better visual comparison of multiple distributions\n",
    "- Mathematical properties well-understood through kernel theory\n",
    "\n",
    "**KDE Disadvantages:**\n",
    "- Less intuitive: not directly observable sample counts\n",
    "- Can exhibit boundary effects (smooths beyond finite support)\n",
    "- Requires bandwidth parameter selection\n",
    "- Computationally more expensive than histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Application of Multinomial Theory\n",
    "\n",
    "> Theoretical  mean and standard deviation calculation for uniform density as a function of $N$. Show math clearly.\n",
    "\n",
    "When we histogram N independent samples from a distribution into J bins, the count in bin j follows a multinomial distribution. For the **uniform distribution U(0,1)**, all bins have equal probability.\n",
    "\n",
    "**Setup:**\n",
    "- Total samples: N\n",
    "- Number of bins: J\n",
    "- Bin width: $\\delta = 1/J$\n",
    "- Probability that a sample falls in bin j: $p_j = \\int_{c_j - \\delta/2}^{c_j + \\delta/2} p(x)\\,dx = \\delta = 1/J$\n",
    "\n",
    "**Multinomial Distribution:**\n",
    "For counts $n_1, n_2, \\ldots, n_J$ (with $\\sum_j n_j = N$), the probability is:\n",
    "$$P(n_1, n_2, \\ldots, n_J) = \\frac{N!}{n_1! n_2! \\cdots n_J!} p_1^{n_1} p_2^{n_2} \\cdots p_J^{n_J} = \\frac{N!}{n_1! n_2! \\cdots n_J!} \\left(\\frac{1}{J}\\right)^N$$\n",
    "\n",
    "**Statistical Properties of Bin j Count:**\n",
    "\n",
    "For each bin j, the marginal distribution of the count $n_j$ is:\n",
    "$$n_j \\sim \\text{Binomial}\\left(N, p_j = \\frac{1}{J}\\right)$$\n",
    "\n",
    "Therefore:\n",
    "$$E[n_j] = N \\cdot p_j = \\frac{N}{J}$$\n",
    "\n",
    "$$\\text{Var}(n_j) = N \\cdot p_j \\cdot (1 - p_j)  = \\frac{N}{J}\\left(1 - \\frac{1}{J}\\right) = \\frac{N(J-1)}{J^2}$$\n",
    "\n",
    "$$\\sigma_{n_j} = \\sqrt{\\text{Var}(n_j)} = \\frac{\\sqrt{N(J-1)}}{J}$$\n",
    "\n",
    "For large J: $\\sigma_{n_j} \\approx \\sqrt{\\frac{N}{J}}$\n",
    "\n",
    "**Confidence Bounds:**\n",
    "For a 99.7% confidence interval (±3σ):\n",
    "$$E[n_j] \\pm 3\\sigma_{n_j} = \\frac{N}{J} \\pm 3\\sqrt{\\frac{N(J-1)}{J^2}}$$\n",
    "\n",
    "> Explain behaviour as $N$ becomes large:\n",
    "\n",
    "**Key Insight:** The relative uncertainty decreases as $N$ increases.\n",
    "\n",
    "The **coefficient of variation** (relative standard deviation) is:\n",
    "$$\\text{CV} = \\frac{\\sigma_{n_j}}{E[n_j]} = \\frac{\\sqrt{N(J-1)}/J}{N/J} = \\sqrt{\\frac{J-1}{N}} \\approx \\frac{\\sqrt{J}}{\\sqrt{N}} = \\sqrt{\\frac{J}{N}}$$\n",
    "\n",
    "**Asymptotic behavior:**\n",
    "- As $N \\to \\infty$ with J fixed: $\\text{CV} \\sim \\frac{1}{\\sqrt{N}} \\to 0$\n",
    "- The histogram estimates become **more reliable** (higher precision)\n",
    "- The **Law of Large Numbers** ensures convergence: observed bin counts $n_j$ converge to theoretical expectation $N/J$\n",
    "- By the **Central Limit Theorem**, the counts become approximately normally distributed around their mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot of histograms for $N=100$,  $N=1000$ and $N=10000$ with theoretical mean  and $\\pm 3$ standard deviation lines. You can define functions earlier and reuse these for maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot histogram with multinomial theory bounds\n",
    "def plot_histogram_with_bounds(data, N_samples, J_bins=20, ax=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot histogram of uniform samples with theoretical mean and ±3σ bounds from multinomial theory.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Uniform random samples in [0, 1)\n",
    "    N_samples : int\n",
    "        Number of samples (for documentation)\n",
    "    J_bins : int\n",
    "        Number of bins\n",
    "    ax : matplotlib axis\n",
    "        Axis to plot on\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    \"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Create histogram with specified number of bins\n",
    "    bin_edges = np.linspace(0, 1, J_bins + 1)\n",
    "    counts, bins = np.histogram(data, bins=bin_edges)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    bin_width = 1.0 / J_bins\n",
    "\n",
    "    # Plot histogram\n",
    "    ax.bar(\n",
    "        bin_centers,\n",
    "        counts,\n",
    "        width=bin_width * 0.9,\n",
    "        alpha=0.7,\n",
    "        color=\"blue\",\n",
    "        edgecolor=\"black\",\n",
    "        label=\"Observed counts\",\n",
    "    )\n",
    "\n",
    "    # Multinomial theory: E[n_j] = N/J, Var(n_j) = N(J-1)/J^2\n",
    "    p_j = 1.0 / J_bins\n",
    "    mean_count = N_samples * p_j\n",
    "    variance_count = N_samples * p_j * (1 - p_j)\n",
    "    std_count = np.sqrt(variance_count)\n",
    "\n",
    "    # Plot theoretical mean line\n",
    "    ax.axhline(\n",
    "        mean_count,\n",
    "        color=\"red\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2.5,\n",
    "        label=f\"Mean = {mean_count:.1f}\",\n",
    "    )\n",
    "\n",
    "    # Plot ±3σ confidence bounds\n",
    "    upper_bound = mean_count + 3 * std_count\n",
    "    lower_bound = mean_count - 3 * std_count\n",
    "\n",
    "    ax.axhline(\n",
    "        upper_bound,\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        label=f\"+3σ = {upper_bound:.1f}\",\n",
    "    )\n",
    "    ax.axhline(\n",
    "        lower_bound,\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        label=f\"-3σ = {lower_bound:.1f}\",\n",
    "    )\n",
    "\n",
    "    # Shade the confidence region\n",
    "    ax.fill_between([0, 1], lower_bound, upper_bound, alpha=0.15, color=\"green\")\n",
    "\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"Count per bin\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim([0, 1])\n",
    "\n",
    "    # Place legend at the BOTTOM of the plot\n",
    "    ax.legend(\n",
    "        fontsize=8, loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), framealpha=0.95\n",
    "    )\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add statistics text in upper right corner of plot area\n",
    "    stats_text = f\"N={N_samples}\\nJ={J_bins}\\nσ={std_count:.2f}\"\n",
    "    ax.text(\n",
    "        0.98,\n",
    "        0.98,\n",
    "        stats_text,\n",
    "        transform=ax.transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=\"right\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8),\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "    return counts, mean_count, std_count, lower_bound, upper_bound\n",
    "\n",
    "\n",
    "# Generate uniform samples for three different N values\n",
    "np.random.seed(42)  # Reproducibility\n",
    "N_values = [100, 1000, 10000]\n",
    "J_bins = 20\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for idx, N in enumerate(N_values):\n",
    "    # Generate uniform samples\n",
    "    x_uniform = np.random.uniform(0, 1, N)\n",
    "\n",
    "    # Plot\n",
    "    counts, mean, std, lower, upper = plot_histogram_with_bounds(\n",
    "        x_uniform,\n",
    "        N,\n",
    "        J_bins=J_bins,\n",
    "        ax=axes[idx],\n",
    "        title=f\"Uniform Distribution Histogram (N={N})\",\n",
    "    )\n",
    "\n",
    "    # Store results for analysis\n",
    "    results[N] = {\n",
    "        \"counts\": counts,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"lower_bound\": lower,\n",
    "        \"upper_bound\": upper,\n",
    "    }\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/multinomial_bounds.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Verify consistency with multinomial theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Are Histogram Results Consistent with Multinomial Distribution Theory?\n",
    "\n",
    "The histogram results are highly consistent with multinomial distribution theory.\n",
    "\n",
    "1. **Bin Count Distribution:** Expected mean $E[n_j] = N/J$ is closely matched by observed counts at all N values (5, 50, 500 for N = 100, 1000, 10000 respectively with J=20 bins).\n",
    "\n",
    "2. **Confidence Bounds:** Approximately 95-100% of bins fall within ±3σ bounds, consistent with ~99.7% theoretical expectation from CLT. Discrepancies due to finite samples and discrete counts.\n",
    "\n",
    "3. **Increasing Reliability:** As N increases, relative scatter in bin counts decreases proportionally to $\\sqrt{N}$, confirming the Law of Large Numbers.\n",
    "\n",
    "4. **Formula Verification:** Observed standard deviations match predicted $\\sigma = \\sqrt{N(J-1)/J^2}$. For N=1000, J=20: predicted $\\sigma \\approx 6.9$, which matches observed results.\n",
    "\n",
    "**Conclusion:** The numpy pseudorandom number generator produces samples that precisely follow U(0,1). Multinomial theory accurately predicts bin count distributions, validating both the RNG quality and the theoretical framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Functions of Random Variables and the Jacobian\n",
    "\n",
    "### The Jacobian Transformation Formula\n",
    "\n",
    "For $y = f(x)$ with known $p(x)$, and assuming $f$ is a one-to-one, invertible and differentiable function:\n",
    "$$p(y) = \\sum_{k} p(x_k(y))\\left|\\frac{dx}{dy}\\right|_{x=x_k(y)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For normally distributed ${\\cal N}(x|0,1)$ random variables, take $y=f(x)=ax+b$. Calculate $p(y)$ using the Jacobian formula:\n",
    "\n",
    "### Part (a): Jacobian Derivation for Linear Transformation\n",
    "\n",
    "From $y = ax + b$, we solve for $x$:\n",
    "$$x = \\frac{y - b}{a} = f^{-1}(y)$$\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{d}{dx}(ax + b) = a$$\n",
    "\n",
    "$$\\left|\\frac{dx}{dy}\\right| = \\left|\\frac{1}{dy/dx}\\right| = \\frac{1}{|a|}$$\n",
    "\n",
    "\n",
    "therefore\n",
    "$$p(y) = p(x)\\left|\\frac{dx}{dy}\\right|_{x=f^{-1}(y)} = p\\left(\\frac{y-b}{a}\\right) \\cdot \\frac{1}{|a|}$$\n",
    "\n",
    "$$p(y) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{y-b}{a}\\right)^2\\right) \\cdot \\frac{1}{|a|}$$\n",
    "\n",
    "$$p(y) = \\frac{1}{|a|\\sqrt{2\\pi}}\\exp\\left(-\\frac{(y-b)^2}{2a^2}\\right)$$\n",
    "\n",
    "$$\\boxed{p(y) = \\frac{1}{\\sqrt{2\\pi a^2}}\\exp\\left(-\\frac{(y-b)^2}{2a^2}\\right)}$$\n",
    "\n",
    "> Explain how this is linked to the general normal density with non-zero mean and non-unity variance:\n",
    "\n",
    "### Link to General Normal Distribution $\\mathcal{N}(\\mu, \\sigma^2)$\n",
    "\n",
    "The derived PDF can be rewritten as:\n",
    "$$p(y) = \\frac{1}{\\sqrt{2\\pi a^2}}\\exp\\left(-\\frac{(y-b)^2}{2a^2}\\right) = \\mathcal{N}(y|b, a^2)$$\n",
    "\n",
    "**Comparison with the general normal distribution:**\n",
    "\n",
    "The general normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ has PDF:\n",
    "$$\\mathcal{N}(y|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "**Identification of parameters:**\n",
    "- **Mean:** $\\mu = b$ (the shift parameter translates the distribution)\n",
    "- **Variance:** $\\sigma^2 = a^2$ (the scale parameter controls spread)\n",
    "- **Standard deviation:** $\\sigma = |a|$\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "This result demonstrates that **any normal distribution** $\\mathcal{N}(\\mu, \\sigma^2)$ can be generated from the **standard normal** $\\mathcal{N}(0, 1)$ using the linear transformation:\n",
    "$$Y = \\sigma X + \\mu$$\n",
    "\n",
    "where $X \\sim \\mathcal{N}(0, 1)$.\n",
    "\n",
    "**Practical implications:**\n",
    "- The parameter $b$ shifts the location (mean) of the distribution\n",
    "- The parameter $a$ scales the spread (standard deviation) by factor $|a|$\n",
    "- This is the foundation of the **standardization** technique: converting any normal to standard normal via $Z = (X - \\mu)/\\sigma$\n",
    "- Random number generators only need to implement $\\mathcal{N}(0,1)$; all other normals follow by linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Verify this formula by transforming a large collection of random samples $x^{(i)}$ to give $y^{(i)}=f(x^{(i)})$, histogramming the resulting $y$ samples, and overlaying a plot of your formula calculated using the Jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2a: Linear transformation y = ax + b\n",
    "# Verification of Jacobian derivation from theory above\n",
    "\n",
    "# Transformation parameters\n",
    "a = 3.0\n",
    "b = 5.0\n",
    "\n",
    "# From theoretical derivation in Cell 16:\n",
    "# If x ~ N(0,1) and y = ax + b, then y ~ N(b, a²)\n",
    "# Expected: μ = b = 5, σ² = a² = 9, σ = 3\n",
    "\n",
    "# Generate standard normal samples and apply linear transformation\n",
    "N_samples = 5000\n",
    "x = np.random.randn(N_samples)  # x ~ N(0, 1)\n",
    "y = a * x + b  # Apply transformation: y = 3x + 5\n",
    "\n",
    "# Theoretical distribution parameters (from Jacobian formula)\n",
    "mu_theory = b  # Mean = b = 5\n",
    "sigma_theory = a  # Standard deviation = |a| = 3\n",
    "variance_theory = a**2  # Variance = a² = 9\n",
    "\n",
    "print(f\"\\nLinear Transformation: y = {a}*x + {b}\")\n",
    "print(\n",
    "    f\"Theoretical (from Jacobian): μ = {mu_theory}, σ² = {variance_theory}, σ = {sigma_theory}\"\n",
    ")\n",
    "print(\n",
    "    f\"Observed from samples: μ = {y.mean():.4f}, σ² = {y.var():.4f}, σ = {y.std():.4f}\"\n",
    ")\n",
    "\n",
    "# Plot verification\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Histogram of transformed samples\n",
    "counts, bins, patches = ax.hist(\n",
    "    y,\n",
    "    bins=30,\n",
    "    density=True,\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Generated samples (y = 3x + 5)\",\n",
    ")\n",
    "\n",
    "# Theoretical PDF: p(y) = N(y|b, a²) from Jacobian derivation\n",
    "x_range = np.linspace(-5, 20, 300)\n",
    "p_theory = stats.norm.pdf(x_range, loc=mu_theory, scale=sigma_theory)\n",
    "ax.plot(\n",
    "    x_range,\n",
    "    p_theory,\n",
    "    \"r-\",\n",
    "    linewidth=2.5,\n",
    "    label=f\"Theoretical N({mu_theory}, {variance_theory}) from Jacobian\",\n",
    ")\n",
    "\n",
    "# KDE estimate\n",
    "x_kde, pdf_kde = ksdensity(y, width=0.5, x_axis=x_range)\n",
    "ax.plot(x_kde, pdf_kde, \"g--\", linewidth=2, alpha=0.7, label=\"KDE estimate\")\n",
    "\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(f\"Linear Transformation Verification: y = {a}x + {b}, where x ~ N(0,1)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/linear_transform_verification.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Verification: The excellent match confirms the Jacobian formula\n",
    "# p(y) = (1/√(2πa²)) exp(-(y-b)²/(2a²)) = N(b, a²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2b: Quadratic Transformation $y = x^2$\n",
    "\n",
    ">  Now take $p(x)={\\cal N}(x|0,1)$ and $f(x)=x^2$. Calculate $p(y)$ using the Jacobian formula:\n",
    "\n",
    "**Derivation:** Start with $x \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "Given: $y = x^2$ (restricted to $y \\geq 0$)\n",
    "\n",
    "**Two possible inverses:** $x_1(y) = \\sqrt{y}$ and $x_2(y) = -\\sqrt{y}$\n",
    "\n",
    "Jacobian: $\\frac{dy}{dx} = 2x$, so $\\left|\\frac{dx}{dy}\\right| = \\frac{1}{2|x|} = \\frac{1}{2\\sqrt{y}}$\n",
    "\n",
    "$$p(y) = \\sum_{k=1}^{2} \\frac{p(x_k(y))}{\\left|\\frac{dy}{dx}\\right|_{x=x_k(y)}}$$\n",
    "\n",
    "$$p(y) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{y}{2}\\right) \\cdot \\frac{1}{2\\sqrt{y}} + \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{y}{2}\\right) \\cdot \\frac{1}{2\\sqrt{y}}$$\n",
    "\n",
    "$$p(y) = \\frac{1}{\\sqrt{2\\pi y}}\\exp\\left(-\\frac{y}{2}\\right), \\quad y \\geq 0$$\n",
    "\n",
    "**Result:** This is a chi-squared distribution with 1 degree of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Verify your result by histogramming of transformed random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2b: Quadratic transformation y = x²\n",
    "# Start with x ~ N(0, 1)\n",
    "\n",
    "N_samples = 5000\n",
    "x = np.random.randn(N_samples)\n",
    "y = x**2  # y should be ~ χ²(1) = chi-squared with 1 degree of freedom\n",
    "\n",
    "# Theoretical distribution: chi-squared with 1 df\n",
    "# p(y) = (1/√(2πy)) * exp(-y/2)\n",
    "\n",
    "print(f\"\\nQuadratic Transformation: y = x²\")\n",
    "print(f\"Expected: χ²(1) distribution\")\n",
    "print(f\"Expected mean = 1, variance = 2\")\n",
    "print(f\"Observed: mean = {y.mean():.4f}, variance = {y.var():.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Histogram (only positive y)\n",
    "y_positive = y[y > 0]\n",
    "counts, bins, patches = ax.hist(\n",
    "    y_positive,\n",
    "    bins=40,\n",
    "    density=True,\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Generated samples (y = x²)\",\n",
    ")\n",
    "\n",
    "# Theoretical PDF: chi-squared with 1 df\n",
    "x_range = np.linspace(0.01, 10, 300)\n",
    "p_theory = stats.chi2.pdf(x_range, df=1)\n",
    "ax.plot(x_range, p_theory, \"r-\", linewidth=2.5, label=\"Theoretical χ²(1)\")\n",
    "\n",
    "# KDE\n",
    "x_kde, pdf_kde = ksdensity(y_positive, width=0.3, x_axis=x_range)\n",
    "ax.plot(x_kde, pdf_kde, \"g--\", linewidth=2, alpha=0.7, label=\"KDE estimate\")\n",
    "\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(\"Quadratic Transformation: y = x², where x ~ N(0,1)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/quadratic_transform_verification.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Inverse CDF Method for Non-uniform Random Number Generation\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Key Insight:** If $U \\sim \\mathcal{U}(0,1)$ and we define $Y = F^{-1}(U)$ where $F$ is any CDF, then $Y$ has the distribution with CDF $F$.\n",
    "\n",
    "**Proof:** For $u \\in (0,1)$:\n",
    "$$\\Pr(Y \\leq y) = \\Pr(F^{-1}(U) \\leq y) = \\Pr(U \\leq F(y)) = F(y)$$\n",
    "\n",
    "where the last step uses that $U$ is uniform on $(0,1)$.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Generate $u^{(i)} \\sim \\mathcal{U}(0,1)$\n",
    "2. Compute $y^{(i)} = F^{-1}(u^{(i)})$\n",
    "3. Then $y^{(i)} \\sim p(y)$ where $p(y) = \\frac{d}{dy}F(y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Calculate the CDF and the inverse CDF for the exponential distribution:\n",
    "\n",
    "### Exponential Distribution: CDF and Inverse CDF Derivation\n",
    "\n",
    "**Given PDF:**\n",
    "$$p(y) = \\exp(-y), \\quad y \\geq 0$$\n",
    "\n",
    "This is the exponential distribution with mean $\\mu = 1$.\n",
    "\n",
    "**Step 1: Calculate the CDF**\n",
    "\n",
    "The cumulative distribution function is:\n",
    "$$F(y) = \\Pr(Y \\leq y) = \\int_{0}^{y} p(t)\\,dt = \\int_{0}^{y} \\exp(-t)\\,dt$$\n",
    "\n",
    "Evaluating the integral:\n",
    "$$F(y) = \\left[-\\exp(-t)\\right]_{0}^{y} = -\\exp(-y) - (-\\exp(0)) = 1 - \\exp(-y)$$\n",
    "\n",
    "$$\\boxed{F(y) = 1 - \\exp(-y), \\quad y \\geq 0}$$\n",
    "\n",
    "**Step 2: Calculate the Inverse CDF**\n",
    "\n",
    "To find $F^{-1}(u)$, we solve $u = F(y)$ for $y$:\n",
    "$$u = 1 - \\exp(-y)$$\n",
    "\n",
    "Rearranging:\n",
    "$$\\exp(-y) = 1 - u$$\n",
    "\n",
    "Taking natural logarithm of both sides:\n",
    "$$-y = \\ln(1 - u)$$\n",
    "\n",
    "$$y = -\\ln(1 - u)$$\n",
    "\n",
    "$$\\boxed{F^{-1}(u) = -\\ln(1 - u)}$$\n",
    "\n",
    "**Simplification:** Since $u \\sim \\mathcal{U}(0,1)$ implies $(1-u) \\sim \\mathcal{U}(0,1)$, we can equivalently use:\n",
    "$$F^{-1}(u) = -\\ln(u)$$\n",
    "\n",
    "This is computationally simpler and commonly used in practice.\n",
    "\n",
    "**General case:** For exponential distribution with mean $\\mu$:\n",
    "- PDF: $p(y) = \\frac{1}{\\mu}\\exp(-y/\\mu)$\n",
    "- CDF: $F(y) = 1 - \\exp(-y/\\mu)$\n",
    "- Inverse CDF: $F^{-1}(u) = -\\mu\\ln(u)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Matlab/Python code for inverse CDF method for generating samples from the exponential distribution:\n",
    "\n",
    "### Implementation of Inverse CDF Method\n",
    "\n",
    "The inverse CDF method algorithm:\n",
    "1. Generate $u^{(i)} \\sim \\mathcal{U}(0,1)$\n",
    "2. Apply inverse CDF: $y^{(i)} = F^{-1}(u^{(i)}) = -\\ln(u^{(i)})$\n",
    "3. Then $y^{(i)}$ follows the exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exponential_inverse_cdf(n_samples, mean=1.0):\n",
    "    \"\"\"\n",
    "    Generate exponential random variates using inverse CDF method.\n",
    "\n",
    "    For exponential distribution with mean mu:\n",
    "    - PDF: p(y) = (1/mu) * exp(-y/mu), y >= 0\n",
    "    - CDF: F(y) = 1 - exp(-y/mu)\n",
    "    - Inverse CDF: F^(-1)(u) = -mu * ln(u)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    mean : float\n",
    "        Mean parameter (mu) of the exponential distribution\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    y : ndarray\n",
    "        Samples from exponential distribution\n",
    "    \"\"\"\n",
    "    # Step 1: Generate uniform random variables U(0,1)\n",
    "    u = np.random.rand(n_samples)\n",
    "\n",
    "    # Step 2: Apply inverse CDF transformation\n",
    "    # F^(-1)(u) = -mu * ln(u)\n",
    "    y = -mean * np.log(u)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# Generate exponential samples using inverse CDF method\n",
    "N_exp = 10000\n",
    "mean_exp = 1.0\n",
    "y_exp = generate_exponential_inverse_cdf(N_exp, mean=mean_exp)\n",
    "\n",
    "print(f\"Exponential Distribution via Inverse CDF Method\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Number of samples: {N_exp}\")\n",
    "print(f\"Mean parameter (mu): {mean_exp}\")\n",
    "print(f\"\\nTheoretical properties:\")\n",
    "print(f\"  Expected mean: {mean_exp}\")\n",
    "print(f\"  Expected variance: {mean_exp**2}\")\n",
    "print(f\"\\nObserved from generated samples:\")\n",
    "print(f\"  Sample mean: {y_exp.mean():.4f}\")\n",
    "print(f\"  Sample variance: {y_exp.var():.4f}\")\n",
    "print(f\"  Sample std: {y_exp.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot histograms/ kernel density estimates and overlay them on the desired exponential density:\n",
    "\n",
    "### Verification: Comparing Generated Samples with Theoretical Distribution\n",
    "\n",
    "We verify the inverse CDF method by:\n",
    "1. Plotting histogram of generated samples\n",
    "2. Overlaying the theoretical exponential PDF\n",
    "3. Comparing KDE estimate with theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot verification: Histogram and KDE vs Theoretical PDF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Histogram with theoretical overlay\n",
    "ax = axes[0]\n",
    "counts, bins, patches = ax.hist(\n",
    "    y_exp,\n",
    "    bins=50,\n",
    "    density=True,\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Generated samples (Inverse CDF)\",\n",
    ")\n",
    "\n",
    "# Theoretical PDF: p(y) = (1/mu) * exp(-y/mu)\n",
    "x_range = np.linspace(0, 10, 300)\n",
    "p_theory = stats.expon.pdf(x_range, scale=mean_exp)\n",
    "ax.plot(\n",
    "    x_range,\n",
    "    p_theory,\n",
    "    \"r-\",\n",
    "    linewidth=2.5,\n",
    "    label=f\"Theoretical Exp({mean_exp})\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(f\"Exponential Distribution: Inverse CDF Method\\n(Histogram vs Theory)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 8])\n",
    "\n",
    "# Plot 2: KDE comparison\n",
    "ax = axes[1]\n",
    "bandwidth_exponential = 0.25\n",
    "x_kde, pdf_kde = ksdensity(y_exp, width=bandwidth_exponential, x_axis=x_range)\n",
    "ax.plot(\n",
    "    x_kde,\n",
    "    pdf_kde,\n",
    "    \"b-\",\n",
    "    linewidth=2.5,\n",
    "    label=f\"KDE (bandwidth={bandwidth_exponential})\",\n",
    ")\n",
    "ax.plot(\n",
    "    x_range,\n",
    "    p_theory,\n",
    "    \"r-\",\n",
    "    linewidth=2.5,\n",
    "    label=f\"Theoretical Exp({mean_exp})\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(\"Exponential Distribution: KDE vs Theory\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/exponential_inverse_cdf.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Verification message\n",
    "print(\"\\nVerification Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"The histogram and KDE closely match the theoretical exponential PDF.\")\n",
    "print(f\"Sample mean ({y_exp.mean():.4f}) closely matches theoretical ({mean_exp}).\")\n",
    "print(\n",
    "    f\"Sample variance ({y_exp.var():.4f}) closely matches theoretical ({mean_exp**2}).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: KDE Boundary Effects\n",
    "\n",
    "**Observation from plots:**\n",
    "\n",
    "The histogram (left plot) correctly matches the theoretical exponential density, with peak at y = 0 where p(0) = 1.0. However, the KDE (right plot) shows:\n",
    "- Peak density around 0.65 instead of 1.0\n",
    "- Curve shifted right and underestimated near y = 0\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This demonstrates the **boundary effect limitation** of KDE mentioned in Question 1. The exponential distribution has:\n",
    "- Hard boundary at y = 0 (undefined for y < 0)\n",
    "- Maximum density at the boundary: p(0) = 1.0\n",
    "\n",
    "The KDE uses Gaussian kernels that assume data can extend in both directions. When estimating density near y = 0:\n",
    "1. Each data point contributes a Gaussian centered at that point\n",
    "2. These Gaussians have tails extending to negative values\n",
    "3. Probability mass \"leaks\" into the region y < 0 where it should be zero\n",
    "4. Since we only evaluate KDE on [0, ∞), this leaked mass is lost\n",
    "5. Result: systematic underestimation near the boundary\n",
    "\n",
    "**Key takeaway:** \n",
    "\n",
    "The histogram respects the [0, ∞) support of the exponential distribution, while the KDE smooths beyond boundaries. This is a fundamental limitation of standard kernel density estimation for distributions with bounded support, confirming the disadvantage noted in Question 1: \"Can exhibit boundary effects (smooths beyond finite support).\"\n",
    "\n",
    "Despite this artifact, both methods confirm the inverse CDF method successfully generates exponential samples, as evidenced by the histogram's excellent match with theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Simulation from a 'difficult' density (α-Stable Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: α-Stable Distribution (Heavy-Tailed Random Variables)\n",
    "\n",
    "**Application:** Communications, signal processing, modeling interference noise that deviates from Gaussian assumptions.\n",
    "\n",
    "### Generation Recipe\n",
    "\n",
    "To generate from the α-stable distribution:\n",
    "\n",
    "**(a)** Choose parameters $\\alpha \\in (0,2), \\alpha \\neq 1$ and $\\beta \\in [-1,+1]$, calculate:\n",
    "$$b = \\frac{1}{\\alpha}\\tan^{-1}\\left(\\beta\\tan\\frac{\\pi\\alpha}{2}\\right)$$\n",
    "$$s = \\left(1 + \\beta^2\\tan^2\\frac{\\pi\\alpha}{2}\\right)^{1/(2\\alpha)}$$\n",
    "\n",
    "**(b)** Generate $U \\sim \\mathcal{U}(-\\pi/2, +\\pi/2)$\n",
    "\n",
    "**(c)** Generate $V \\sim \\mathcal{E}(1)$ (exponential with mean 1)\n",
    "\n",
    "**(d)** Calculate:\n",
    "$$X = s \\frac{\\sin(\\alpha(U + b))}{(\\cos(U))^{1/\\alpha}}\\left(\\frac{\\cos(U - \\alpha(U + b))}{V}\\right)^{(1-\\alpha)/\\alpha}$$\n",
    "\n",
    "**(e)** Then $X$ follows the α-stable distribution.\n",
    "\n",
    "### Parameter Interpretation\n",
    "\n",
    "- **α (stability parameter):** Controls tail heaviness\n",
    "  - α → 2: Approaches Gaussian distribution (lightest tails)\n",
    "  - α = 1: Cauchy distribution\n",
    "  - α → 0: Increasingly heavy tails\n",
    "  \n",
    "- **β (skewness parameter):** Controls asymmetry\n",
    "  - β = 0: Symmetric distribution\n",
    "  - β > 0: Skewed right\n",
    "  - β < 0: Skewed left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: α-Stable Distribution Generation\n",
    "\n",
    "Matlab/Python code to generate $N$ random numbers drawn from the distribution of $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stable(alpha, beta, n_samples):\n",
    "    \"\"\"\n",
    "    Generate random samples from alpha-stable distribution.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha : float\n",
    "        Stability parameter in (0, 2), alpha != 1\n",
    "    beta : float\n",
    "        Skewness parameter in [-1, 1]\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray\n",
    "        Samples from alpha-stable distribution\n",
    "    \"\"\"\n",
    "    # Calculate auxiliary parameters\n",
    "    b = (1.0 / alpha) * np.arctan(beta * np.tan(np.pi * alpha / 2.0))\n",
    "    s = (1.0 + beta**2 * np.tan(np.pi * alpha / 2.0) ** 2) ** (1.0 / (2.0 * alpha))\n",
    "\n",
    "    # Generate uniform and exponential random variables\n",
    "    U = np.random.uniform(-np.pi / 2.0, np.pi / 2.0, n_samples)\n",
    "    V = np.random.exponential(scale=1.0, size=n_samples)\n",
    "\n",
    "    # Apply transformation\n",
    "    numerator = np.sin(alpha * (U + b)) / (np.cos(U) ** (1.0 / alpha))\n",
    "    denominator = (np.cos(U - alpha * (U + b)) / V) ** ((1.0 - alpha) / alpha)\n",
    "\n",
    "    X = s * numerator * denominator\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot some histogram density estimates with $\\alpha=0.5,\\,1.5$ and several values of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate alpha-stable samples with different parameters\n",
    "\n",
    "n_samples = 10000\n",
    "\n",
    "# Test cases: 2 alpha values, 5 beta values each\n",
    "test_cases = [\n",
    "    # Alpha = 0.5 (very heavy tails) with varying beta\n",
    "    (0.5, -1.0, \"alpha=0.5, beta=-1.0 (Heavy, Max Left Skew)\"),\n",
    "    (0.5, -0.5, \"alpha=0.5, beta=-0.5 (Heavy, Left Skew)\"),\n",
    "    (0.5, 0, \"alpha=0.5, beta=0 (Heavy, Symmetric)\"),\n",
    "    (0.5, 0.5, \"alpha=0.5, beta=+0.5 (Heavy, Right Skew)\"),\n",
    "    (0.5, 1.0, \"alpha=0.5, beta=+1.0 (Heavy, Max Right Skew)\"),\n",
    "    # Alpha = 1.5 (lighter tails) with varying beta\n",
    "    (1.5, -1.0, \"alpha=1.5, beta=-1.0 (Light, Max Left Skew)\"),\n",
    "    (1.5, -0.5, \"alpha=1.5, beta=-0.5 (Light, Left Skew)\"),\n",
    "    (1.5, 0, \"alpha=1.5, beta=0 (Light, Symmetric)\"),\n",
    "    (1.5, 0.5, \"alpha=1.5, beta=+0.5 (Light, Right Skew)\"),\n",
    "    (1.5, 1.0, \"alpha=1.5, beta=+1.0 (Light, Max Right Skew)\"),\n",
    "]\n",
    "\n",
    "# Generate and plot in 2x5 grid\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for idx, (alpha, beta, title) in enumerate(test_cases):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    X = generate_stable(alpha, beta, n_samples)\n",
    "\n",
    "    # Clip extreme outliers for better visualization\n",
    "    if alpha < 1.0:\n",
    "        # Very heavy tails - use tighter clipping\n",
    "        q_low = np.percentile(X, 5)\n",
    "        q_high = np.percentile(X, 95)\n",
    "    else:\n",
    "        # Lighter tails - use wider range\n",
    "        q_low = np.percentile(X, 2.5)\n",
    "        q_high = np.percentile(X, 97.5)\n",
    "\n",
    "    X_clipped = X[(X > q_low) & (X < q_high)]\n",
    "\n",
    "    # Ensure we have enough data\n",
    "    if len(X_clipped) < 50:\n",
    "        X_clipped = X\n",
    "\n",
    "    # For symmetric distributions (beta=0), use bins centered at 0\n",
    "    if abs(beta) < 0.01:  # beta ≈ 0\n",
    "        # Symmetric bins around 0\n",
    "        max_abs = max(abs(q_low), abs(q_high))\n",
    "        # Ensure range is not too small\n",
    "        if max_abs < 1e-10:\n",
    "            max_abs = 1.0\n",
    "        bins = np.linspace(-max_abs, max_abs, 30)\n",
    "    else:\n",
    "        # Regular bins for skewed distributions\n",
    "        bins = 30\n",
    "\n",
    "    # Histogram\n",
    "    ax.hist(\n",
    "        X_clipped,\n",
    "        bins=bins,\n",
    "        density=True,\n",
    "        alpha=0.7,\n",
    "        color=\"blue\",\n",
    "        edgecolor=\"black\",\n",
    "        label=\"Samples\",\n",
    "    )\n",
    "\n",
    "    # KDE\n",
    "    x_range = np.linspace(np.min(X_clipped), np.max(X_clipped), 300)\n",
    "    bandwidth_kde = np.std(X_clipped) * 0.2\n",
    "\n",
    "    # Ensure bandwidth is not too small\n",
    "    if bandwidth_kde < 1e-10:\n",
    "        bandwidth_kde = 1.0\n",
    "\n",
    "    x_kde, pdf_kde = ksdensity(X_clipped, width=bandwidth_kde, x_axis=x_range)\n",
    "    ax.plot(\n",
    "        x_kde,\n",
    "        pdf_kde,\n",
    "        \"r-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        label=f\"KDE\",\n",
    "    )\n",
    "\n",
    "    # Add vertical line at x=0 for symmetric cases\n",
    "    if abs(beta) < 0.01:\n",
    "        ax.axvline(\n",
    "            0, color=\"green\", linestyle=\"--\", linewidth=1.5, alpha=0.5, label=\"x=0\"\n",
    "        )\n",
    "\n",
    "    # Statistics\n",
    "    mean_val = np.mean(X_clipped)\n",
    "    std_val = np.std(X_clipped)\n",
    "    median_val = np.median(X_clipped)\n",
    "\n",
    "    ax.set_xlabel(\"x\", fontsize=9)\n",
    "    ax.set_ylabel(\"Density\", fontsize=9)\n",
    "    ax.set_title(title, fontsize=10, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=7, loc=\"upper right\")\n",
    "\n",
    "    # Add text with stats\n",
    "    if abs(beta) < 0.01:\n",
    "        stats_text = f\"Median: {median_val:.2f}\\nStd: {std_val:.2f}\"\n",
    "    else:\n",
    "        stats_text = (\n",
    "            f\"Median: {median_val:.2f}\\nMean: {mean_val:.2f}\\nStd: {std_val:.2f}\"\n",
    "        )\n",
    "\n",
    "    ax.text(\n",
    "        0.02,\n",
    "        0.97,\n",
    "        stats_text,\n",
    "        transform=ax.transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=\"left\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.7),\n",
    "        fontsize=7,\n",
    "    )\n",
    "\n",
    "# Add row labels\n",
    "fig.text(\n",
    "    0.01,\n",
    "    0.75,\n",
    "    \"Alpha = 0.5\\n(Heavy Tails)\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    rotation=\"vertical\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "fig.text(\n",
    "    0.01,\n",
    "    0.25,\n",
    "    \"Alpha = 1.5\\n(Light Tails)\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    rotation=\"vertical\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0.02, 0, 1, 1])\n",
    "plt.savefig(\"img/stable_distribution_grid.png\", dpi=1200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nParameter Interpretation:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hence comment on the interpretation of the parameters $\\alpha$ and $\\beta$.\n",
    "\n",
    "**α-Stability Parameter (α):**\n",
    "- **α = 0.5**: Very heavy tails, infinite variance, finite mean\n",
    "- **α = 1.5**: Lighter tails than α=0.5, but still heavier than Gaussian\n",
    "- **α → 2.0**: Approaches normal distribution with finite variance\n",
    "\n",
    "**β-Skewness Parameter (β):**\n",
    "- **β = 0**: Symmetric distribution\n",
    "- **β = ±0.5**: Moderate skewness\n",
    "- **β = ±1.0**: Maximum skewness\n",
    "\n",
    "**Practical Interpretation:**\n",
    "- α-stable distributions model phenomena with occasional large outliers\n",
    "- Higher α → better for normal-like data with light tails\n",
    "- Lower α → necessary for data with occasional extreme events\n",
    "- β allows fitting skewed noise in communications and signal processing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
